{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied Brats17_2013_10_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_11_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_12_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_13_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_14_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_17_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_18_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_19_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_20_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_21_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_22_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_23_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_25_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_26_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_27_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_2_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_3_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_4_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_5_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_2013_7_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AAB_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AAG_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AAL_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AAP_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ABB_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ABE_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ABM_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ABN_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ABO_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ABY_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ALN_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ALU_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ALX_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AME_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AMH_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ANG_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ANI_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ANP_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ANZ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AOD_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AOH_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AOO_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AOP_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AOZ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_APR_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_APY_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_APZ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQA_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQD_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQG_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQJ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQN_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQO_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQP_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQQ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQR_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQT_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQU_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQV_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQY_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AQZ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ARF_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ARW_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ARZ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASA_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASE_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASG_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASH_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASK_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASN_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASO_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASU_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASV_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASW_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ASY_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ATB_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ATD_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ATF_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ATP_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ATV_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_ATX_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AUN_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AUQ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AUR_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AVG_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AVJ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AVV_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AWG_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AWH_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AWI_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AXJ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AXL_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AXM_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AXN_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AXO_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AXQ_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AXW_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AYA_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AYI_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AYU_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AYW_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AZD_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_AZH_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_BFB_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_BFP_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_BHB_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_BHK_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_CBICA_BHM_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_105_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_111_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_113_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_117_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_118_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_121_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_131_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_133_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_135_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_138_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_147_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_149_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_150_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_151_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_162_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_165_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_167_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_168_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_171_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_179_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_180_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_184_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_186_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_190_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_192_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_198_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_199_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_201_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_203_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_205_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_208_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_211_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_218_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_221_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_222_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_226_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_231_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_234_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_235_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_242_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_247_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_257_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_265_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_274_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_277_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_278_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_280_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_283_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_290_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_296_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_300_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_309_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_314_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_319_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_321_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_322_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_328_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_331_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_332_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_335_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_338_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_343_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_361_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_368_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_370_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_372_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_374_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_375_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_377_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_378_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_390_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_394_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_396_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_401_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_406_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_409_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_411_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_412_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_419_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_425_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_429_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_430_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_436_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_437_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_444_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_448_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_455_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_460_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_469_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_471_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_473_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_474_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_478_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_479_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_491_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_498_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_499_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_603_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_605_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_606_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_607_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Copied Brats17_TCIA_608_1_t1ce.nii to /Volumes/toby/BRATS2017/TrainingDataset/images\n",
      "Finished copying t1ce.nii files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the base path and destination path\n",
    "base_path = \"/Volumes/toby/BRATS2017/Brats17TrainingData/HGG\"\n",
    "destination_path = \"/Volumes/toby/BRATS2017/TrainingDataset/images\"\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# Loop through the subfolders in the base path\n",
    "for patient_folder in os.listdir(base_path):\n",
    "    patient_path = os.path.join(base_path, patient_folder)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(patient_path):\n",
    "        # Loop through the files in the patient folder\n",
    "        for file_name in os.listdir(patient_path):\n",
    "            if file_name.endswith(\"t1ce.nii\"):\n",
    "                # Construct full file path\n",
    "                file_path = os.path.join(patient_path, file_name)\n",
    "                \n",
    "                # Copy the file to the destination folder\n",
    "                shutil.copy(file_path, destination_path)\n",
    "                print(f\"Copied {file_name} to {destination_path}\")\n",
    "\n",
    "print(\"Finished copying t1ce.nii files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBMdataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_path, target_dimensions=(128, 128, 128), target_spacing=(1, 1, 1), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images.\n",
    "            csv_path (str): Path to the CSV file containing patient metadata.\n",
    "            target_dimensions (tuple): Desired output image dimensions (e.g., 128x128x128).\n",
    "            target_spacing (tuple): Target voxel spacing for resampling (e.g., 1x1x1 mm).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.patient_data = self._load_patient_data(csv_path)\n",
    "        self.patient_ids = list(self.patient_data.keys())\n",
    "        self.target_dimensions = target_dimensions\n",
    "        self.target_spacing = target_spacing\n",
    "\n",
    "    def _load_patient_data(self, csv_path):\n",
    "        data = pd.read_csv(csv_path)\n",
    "        patient_data = {}\n",
    "        for _, row in data.iterrows():\n",
    "            patient_id = row['Brats17ID']\n",
    "            age = row['Age']\n",
    "            survival = row['Survival']\n",
    "            patient_data[patient_id] = {'Age': age, 'Survival': survival}\n",
    "        return patient_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "    \n",
    "    def _resample_image(self, image_path):\n",
    "        \"\"\"Resample the image to the target voxel spacing using torchio.\"\"\"\n",
    "        image = tio.ScalarImage(image_path)\n",
    "        resample_transform = tio.transforms.Resample(self.target_spacing)\n",
    "        resampled_image = resample_transform(image)\n",
    "        return resampled_image\n",
    "    \n",
    "    def _resize_image(self, image):\n",
    "        \"\"\"Resize the image to the target dimensions (e.g., 128x128x128).\"\"\"\n",
    "        resize_transform = tio.transforms.Resize(self.target_dimensions)\n",
    "        resized_image = resize_transform(image)\n",
    "        return resized_image.data.numpy()\n",
    "    \n",
    "    def _normalize_image(self, image):\n",
    "        \"\"\"Normalize the image by subtracting mean and dividing by std.\"\"\"\n",
    "        mean = np.mean(image)\n",
    "        std = np.std(image)\n",
    "        if std == 0:  # To avoid division by zero\n",
    "            std = 1.0\n",
    "        return (image - mean) / std\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the patient ID\n",
    "        patient_id = self.patient_ids[idx]\n",
    "\n",
    "        # Construct the file paths for the MRI images and segmentation\n",
    "        t1_path = os.path.join(self.image_dir, f\"{patient_id}_t1.nii\")\n",
    "        t1ce_path = os.path.join(self.image_dir, f\"{patient_id}_t1ce.nii\")\n",
    "        flair_path = os.path.join(self.image_dir, f\"{patient_id}_flair.nii\")\n",
    "        t2_path = os.path.join(self.image_dir, f\"{patient_id}_t2.nii\")\n",
    "        seg_path = os.path.join(self.image_dir, f\"{patient_id}_seg.nii\")\n",
    "        \n",
    "        # Load and resample the images\n",
    "        t1 = self._resample_image(t1_path)\n",
    "        t1ce = self._resample_image(t1ce_path)\n",
    "        flair = self._resample_image(flair_path)\n",
    "        t2 = self._resample_image(t2_path)\n",
    "        seg = self._resample_image(seg_path)\n",
    "\n",
    "        # Resize the images to target dimensions (e.g., 128x128x128)\n",
    "        t1 = self._resize_image(t1)\n",
    "        t1ce = self._resize_image(t1ce)\n",
    "        flair = self._resize_image(flair)\n",
    "        t2 = self._resize_image(t2)\n",
    "        seg = self._resize_image(seg)\n",
    "\n",
    "        # Print the shape of the image after resampling and resizing\n",
    "        #print(f\"Image shape after resampling and resizing: {t1.shape}\")\n",
    "        \n",
    "        # Normalize the images (for each modality)\n",
    "        t1 = self._normalize_image(t1)\n",
    "        t1ce = self._normalize_image(t1ce)\n",
    "        flair = self._normalize_image(flair)\n",
    "        t2 = self._normalize_image(t2)\n",
    "        \n",
    "        # Stack the images and segmentation into a single tensor\n",
    "        image = np.stack([t1, t1ce, flair, t2, seg], axis=0)\n",
    "\n",
    "        # Print the size of the image after resampling, resizing, and normalization\n",
    "        #print(f\"Image shape after resampling, resizing, and normalization: {image.shape}\")\n",
    "        \n",
    "        # Get the survival time for this patient\n",
    "        survival_time = self.patient_data[patient_id]['Survival']\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Convert image and survival time to torch tensors\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        survival_time = torch.tensor(survival_time, dtype=torch.float32)\n",
    "        \n",
    "        return image, survival_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Batch 1\n",
      "Images shape: torch.Size([4, 5, 1, 128, 128, 128])\n",
      "Survival times: tensor([ 329.,  317., 1337.,  473.])\n",
      "Batch 2\n",
      "Images shape: torch.Size([4, 5, 1, 128, 128, 128])\n",
      "Survival times: tensor([1489.,  268.,  355., 1458.])\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n",
      "Image shape after resampling and resizing: (1, 128, 128, 128)\n",
      "Image shape after resampling, resizing, and normalization: (5, 1, 128, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the directory with images and segmentation\n",
    "image_dir = \"/Volumes/toby/BRATS2017/TrainingDataset/images\"\n",
    "\n",
    "# Define the path to the CSV file containing the patient survival data\n",
    "csv_path = \"/Volumes/toby/BRATS2017/Brats17TrainingData/survival_data.csv\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = GBMdataset(image_dir=image_dir, csv_path=csv_path)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "# Example loop over the data\n",
    "for batch_idx, (images, survival_times) in enumerate(dataloader):\n",
    "    if batch_idx < 2:  # Only print for the first 2 batches\n",
    "        print(f\"Batch {batch_idx + 1}\")\n",
    "        print(f\"Images shape: {images.shape}\")\n",
    "        print(f\"Survival times: {survival_times}\")\n",
    "    else:\n",
    "        break  # Exit the loop after the first 2 batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class GaussianNoise3D(nn.Module):\n",
    "    def __init__(self, mean=0.0, std=0.1):\n",
    "        super(GaussianNoise3D, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:  # Apply noise only during training\n",
    "            noise = torch.randn_like(x) * self.std + self.mean\n",
    "            return x + noise\n",
    "        return x\n",
    "    \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_shape, network_depth, no_convolutions, conv_filter_no_init, \n",
    "                 conv_kernel_size, latent_representation_dim, l1, l2, dropout_value, \n",
    "                 use_batch_normalization, activation, gaussian_noise_std=None):\n",
    "        super(encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.network_depth = network_depth\n",
    "        self.no_convolutions = no_convolutions\n",
    "        self.conv_filter_no_init = conv_filter_no_init\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.latent_representation_dim = latent_representation_dim\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.dropout_value = dropout_value\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "        self.activation = activation\n",
    "        self.gaussian_noise_std = gaussian_noise_std   \n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "\n",
    "        # Gaussian noise layer\n",
    "        if gaussian_noise_std:\n",
    "            self.noise_layer = GaussianNoise3D(gaussian_noise_std)\n",
    "        else:\n",
    "            self.noise_layer = None\n",
    "\n",
    "        # Convolutional layers\n",
    "        in_channels = input_shape[0]\n",
    "        for i in range(network_depth):\n",
    "            for j in range(no_convolutions):\n",
    "                out_channels = self.conv_filter_no_init * (2 ** i)\n",
    "                conv_layer = nn.Conv3d(in_channels, out_channels, conv_kernel_size, padding=conv_kernel_size // 2)\n",
    "                self.encoder_layers.append(conv_layer)\n",
    "                if self.use_batch_normalization:\n",
    "                    self.encoder_layers.append(nn.BatchNorm3d(out_channels))\n",
    "                if self.activation == 'leakyrelu':\n",
    "                    self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                else:\n",
    "                    self.encoder_layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = out_channels\n",
    "            self.encoder_layers.append(nn.MaxPool3d(kernel_size=2, stride=2))\n",
    "            if dropout_value:\n",
    "                self.encoder_layers.append(nn.Dropout3d(p=dropout_value))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate feature map size after convolution\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            conv_output = self._forward_conv_layers(dummy_input)\n",
    "            self.feature_map_size = conv_output.size()\n",
    "            flattened_dim = conv_output.view(1, -1).size(1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(flattened_dim, latent_representation_dim)\n",
    "        if activation == 'leakyrelu':\n",
    "            self.activation_fn = nn.LeakyReLU(inplace=True)\n",
    "        else:\n",
    "            self.activation_fn = nn.ReLU(inplace=True)\n",
    "            \n",
    "    def _forward_conv_layers(self, x):\n",
    "        if self.noise_layer:\n",
    "            x = self.noise_layer(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_shape = (5, 128, 128, 128)  # 5 channels, 240x240x155 spatial dimensions\n",
    "network_depth = 4\n",
    "no_convolutions = 2\n",
    "conv_filter_no_init = 32\n",
    "conv_kernel_size = 3\n",
    "latent_representation_dim = 128\n",
    "l1 = 0.01\n",
    "l2 = 0.01\n",
    "dropout_value = 0.5\n",
    "use_batch_normalization = True\n",
    "activation = 'leakyrelu'\n",
    "gaussian_noise_std = 0.1\n",
    "\n",
    "encoder = encoder(\n",
    "    input_shape=input_shape,\n",
    "    network_depth=network_depth,\n",
    "    no_convolutions=no_convolutions,\n",
    "    conv_filter_no_init=conv_filter_no_init,\n",
    "    conv_kernel_size=conv_kernel_size,\n",
    "    latent_representation_dim=latent_representation_dim,\n",
    "    l1=l1,\n",
    "    l2=l2,\n",
    "    dropout_value=dropout_value,\n",
    "    use_batch_normalization=use_batch_normalization,\n",
    "    activation=activation,\n",
    "    gaussian_noise_std=gaussian_noise_std\n",
    ")\n",
    "\n",
    "print(encoder.feature_map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3-0: in_channels = 256, out_channels = 512\n",
      "Layer 3-1: in_channels = 512, out_channels = 512\n",
      "Layer 2-0: in_channels = 512, out_channels = 256\n",
      "Layer 2-1: in_channels = 256, out_channels = 256\n",
      "Layer 1-0: in_channels = 256, out_channels = 128\n",
      "Layer 1-1: in_channels = 128, out_channels = 128\n",
      "Layer 0-0: in_channels = 128, out_channels = 64\n",
      "Layer 0-1: in_channels = 64, out_channels = 64\n",
      "Final Layer: in_channels = 64, out_channels = 5\n",
      "Decoder output shape: torch.Size([1, 5, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def downsample_fn(depth):\n",
    "    if depth == 4:\n",
    "        return [(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)]  # Increase depth upsample factor to 2\n",
    "    elif depth == 2:\n",
    "        return [(4, 4, 4), (4, 4, 4)]  # Increase depth upsample factor to 4 if needed\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported depth: {depth}')\n",
    "    \n",
    "class Decoder3D(nn.Module):\n",
    "    def __init__(self, conv_shape, network_depth, no_convolutions, conv_filter_no_init,\n",
    "                 conv_kernel_size, latent_representation_dim, output_channels=5, l1=0.0, l2=0.0,\n",
    "                 dropout_value=0.0, use_batch_normalization=False, activation='relu'):\n",
    "        super(Decoder3D, self).__init__()\n",
    "        self.conv_shape = conv_shape  # Shape of the feature map at the start of the decoder\n",
    "        self.network_depth = network_depth\n",
    "        self.no_convolutions = no_convolutions\n",
    "        self.conv_filter_no_init = conv_filter_no_init\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.latent_representation_dim = latent_representation_dim\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.dropout_value = dropout_value\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "        self.activation = activation\n",
    "        self.output_channels = output_channels  # Final output channels (e.g., 5 channels for MRI modalities)\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'leakyrelu':\n",
    "            self.activation_fn = nn.LeakyReLU(inplace=True)\n",
    "        else:\n",
    "            self.activation_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Fully connected layer to reshape the latent vector into a 3D shape\n",
    "        self.fc = nn.Linear(latent_representation_dim, np.prod(self.conv_shape))\n",
    "\n",
    "        # Reshape layer to convert the flat output of the FC layer into a 3D volume\n",
    "        self.reshape = lambda x: x.view(-1, *self.conv_shape)\n",
    "        \n",
    "        # Decoder layers (upsample and conv layers)\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        # Reverse the depth, so we progressively upsample back to the original image size\n",
    "        in_channels = conv_shape[0]  # Start with the number of channels from the conv_shape\n",
    "        for i in reversed(range(network_depth)):\n",
    "            # Upsampling layer\n",
    "            upsample_factors = downsample_fn(network_depth)[i]\n",
    "            self.decoder_layers.append(nn.Upsample(scale_factor=upsample_factors, mode='trilinear', align_corners=False))\n",
    "            \n",
    "            # Convolution layers\n",
    "            out_channels = self.conv_filter_no_init * (2 ** i)  # Reduce the number of channels as we move up the network\n",
    "            for j in range(no_convolutions):\n",
    "                print(f\"Layer {i}-{j}: in_channels = {in_channels}, out_channels = {out_channels}\")\n",
    "                self.decoder_layers.append(nn.Conv3d(in_channels, out_channels, kernel_size=conv_kernel_size, padding=1))\n",
    "                if use_batch_normalization:\n",
    "                    self.decoder_layers.append(nn.BatchNorm3d(out_channels))\n",
    "                if activation == 'leakyrelu':\n",
    "                    self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                else:\n",
    "                    self.decoder_layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = out_channels  # Update in_channels for the next layer\n",
    "            if dropout_value > 0.0:\n",
    "                self.decoder_layers.append(nn.Dropout3d(p=dropout_value))\n",
    "        \n",
    "        # Final convolution to produce the reconstructed image with the correct number of output channels\n",
    "        print(f\"Final Layer: in_channels = {in_channels}, out_channels = {self.output_channels}\")\n",
    "        self.final_conv = nn.Conv3d(in_channels, self.output_channels, conv_kernel_size, padding=1)\n",
    "        self.final_activation = nn.ReLU()  # You could change this to `nn.Sigmoid()` or `nn.Tanh()` depending on the data range\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the latent vector via the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Reshape to the shape required for the convolutional layers\n",
    "        x = self.reshape(x)  # Reshape to 3D tensor (batch_size, channels, depth, height, width)\n",
    "        \n",
    "        # Apply the decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Final convolution to produce the output volume\n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_activation(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "latent_vector = torch.randn(1, 128)  # Latent vector of size [batch_size, latent_representation_dim]\n",
    "conv_shape = (256, 8, 8, 8)  # Example shape of the feature map before the decoder\n",
    "decoder_model = Decoder3D(conv_shape=conv_shape, network_depth=4, no_convolutions=2, \n",
    "                          conv_filter_no_init=64, conv_kernel_size=3, \n",
    "                          latent_representation_dim=128)\n",
    "\n",
    "output_image = decoder_model(latent_vector)\n",
    "print(\"Decoder output shape:\", output_image.shape)  # Should be [batch_size, 5, 240, 240, 150] or similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentParametersModel(nn.Module):\n",
    "    def __init__(self, latent_representation_dim, l1=0.0, l2=0.0):\n",
    "        super(LatentParametersModel, self).__init__()\n",
    "        self.mu_sigma_layer = nn.Linear(\n",
    "            in_features=latent_representation_dim, \n",
    "            out_features=2\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.mu_sigma_layer.weight)\n",
    "        nn.init.zeros_(self.mu_sigma_layer.bias)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu_sigma = self.mu_sigma_layer(x)\n",
    "        return mu_sigma\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted mu and sigma: tensor([[ 1.1835, -2.3457]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "latent_representation_dim = 128  # Example latent representation dimension\n",
    "model = LatentParametersModel(latent_representation_dim=latent_representation_dim)\n",
    "\n",
    "# Simulate an input from the encoder\n",
    "x = torch.randn((1, latent_representation_dim))  # Batch size of 1, with 128-dimensional latent vector\n",
    "mu_sigma = model(x)\n",
    "\n",
    "print(\"Predicted mu and sigma:\", mu_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4084, 0.2187, 0.4220])\n"
     ]
    }
   ],
   "source": [
    "def reconstruction_loss(y_true, y_pred):\n",
    "    mse_loss = F.mse_loss(y_pred, y_true, reduction='none')\n",
    "    reduced_loss = mse_loss.mean(dim=[1, 2, 3])\n",
    "    return reduced_loss\n",
    "\n",
    "import torch\n",
    "\n",
    "def survival_loss(mu, sigma, x, delta):\n",
    "    \"\"\"\n",
    "    Custom loss function based on the negative log-likelihood.\n",
    "\n",
    "    :param mu: Predicted mean (log of hazard ratio), tensor of shape (batch_size,)\n",
    "    :param sigma: Predicted standard deviation (scale parameter), tensor of shape (batch_size,)\n",
    "    :param x: Observed time (log-transformed), tensor of shape (batch_size,)\n",
    "    :param delta: Event indicator (1 if event occurred, 0 if censored), tensor of shape (batch_size,)\n",
    "    :return: Computed loss, scalar value\n",
    "    \"\"\"\n",
    "    # Negative log-likelihood term\n",
    "    total_loss = -(torch.log(x)-mu)/sigma.sum()+(delta * torch.log(sigma) + (1 + delta) * torch.log(1 + torch.exp((torch.log(x)-mu)/sigma)))\n",
    "    \n",
    "    # Return the mean loss across the batch\n",
    "    return total_loss / x.size(0)\n",
    "\n",
    "# Example usage\n",
    "mu = torch.tensor([0.5, 0.8, 0.3])  # Predicted means (log hazard ratios)\n",
    "sigma = torch.tensor([1.1, 1.2, 1.1])  # Predicted standard deviations (not log-transformed)\n",
    "x = torch.tensor([1.0, 0.8, 0.9])  # Log-transformed observed times\n",
    "delta = torch.tensor([1.0, 0.0, 1.0])  # Event indicators\n",
    "\n",
    "loss = survival_loss(mu, sigma, x, delta)\n",
    "print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
