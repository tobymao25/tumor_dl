{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # link to data \n",
    "# # https://www.kaggle.com/datasets/abdullahalmunem/brats17/data\n",
    "\n",
    "# # Define the base path and destination path\n",
    "# base_path = \"/Volumes/toby/BRATS2017/Brats17TrainingData/HGG\"\n",
    "# destination_path = \"/Volumes/toby/BRATS2017/TrainingDataset/images\"\n",
    "\n",
    "# # Ensure the destination directory exists\n",
    "# os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# # Loop through the subfolders in the base path\n",
    "# for patient_folder in os.listdir(base_path):\n",
    "#     patient_path = os.path.join(base_path, patient_folder)\n",
    "    \n",
    "#     # Check if it's a directory\n",
    "#     if os.path.isdir(patient_path):\n",
    "#         # Loop through the files in the patient folder\n",
    "#         for file_name in os.listdir(patient_path):\n",
    "#             if file_name.endswith(\"t1ce.nii\"):\n",
    "#                 # Construct full file path\n",
    "#                 file_path = os.path.join(patient_path, file_name)\n",
    "                \n",
    "#                 # Copy the file to the destination folder\n",
    "#                 shutil.copy(file_path, destination_path)\n",
    "#                 print(f\"Copied {file_name} to {destination_path}\")\n",
    "\n",
    "# print(\"Finished copying t1ce.nii files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchio as tio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBMdataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_path, target_dimensions=(128, 128, 128), target_spacing=(1, 1, 1), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images.\n",
    "            csv_path (str): Path to the CSV file containing patient metadata.\n",
    "            target_dimensions (tuple): Desired output image dimensions (e.g., 128x128x128).\n",
    "            target_spacing (tuple): Target voxel spacing for resampling (e.g., 1x1x1 mm).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.patient_data = self._load_patient_data(csv_path)\n",
    "        self.patient_ids = list(self.patient_data.keys())\n",
    "        self.target_dimensions = target_dimensions\n",
    "        self.target_spacing = target_spacing\n",
    "\n",
    "    def _load_patient_data(self, csv_path):\n",
    "        data = pd.read_csv(csv_path)\n",
    "        patient_data = {}\n",
    "        for _, row in data.iterrows():\n",
    "            patient_id = row['Brats17ID']\n",
    "            age = row['Age']\n",
    "            survival = row['Survival']\n",
    "            patient_data[patient_id] = {'Age': age, 'Survival': survival}\n",
    "        return patient_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "    \n",
    "    def _resample_image(self, image_path):\n",
    "        \"\"\"Resample the image to the target voxel spacing using torchio.\"\"\"\n",
    "        image = tio.ScalarImage(image_path)\n",
    "        resample_transform = tio.transforms.Resample(self.target_spacing)\n",
    "        resampled_image = resample_transform(image)\n",
    "        return resampled_image\n",
    "    \n",
    "    def _resize_image(self, image):\n",
    "        \"\"\"Resize the image to the target dimensions (e.g., 128x128x128).\"\"\"\n",
    "        resize_transform = tio.transforms.Resize(self.target_dimensions)\n",
    "        resized_image = resize_transform(image)\n",
    "        return resized_image.data.numpy()\n",
    "    \n",
    "    def _normalize_image(self, image):\n",
    "        \"\"\"Normalize the image by subtracting mean and dividing by std.\"\"\"\n",
    "        mean = np.mean(image)\n",
    "        std = np.std(image)\n",
    "        if std == 0:  # To avoid division by zero\n",
    "            std = 1.0\n",
    "        return (image - mean) / std\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the patient ID\n",
    "        patient_id = self.patient_ids[idx]\n",
    "\n",
    "        # Construct the file paths for the MRI images and segmentation\n",
    "        t1_path = os.path.join(self.image_dir, f\"{patient_id}_t1.nii\")\n",
    "        t1ce_path = os.path.join(self.image_dir, f\"{patient_id}_t1ce.nii\")\n",
    "        flair_path = os.path.join(self.image_dir, f\"{patient_id}_flair.nii\")\n",
    "        t2_path = os.path.join(self.image_dir, f\"{patient_id}_t2.nii\")\n",
    "        seg_path = os.path.join(self.image_dir, f\"{patient_id}_seg.nii\")\n",
    "        \n",
    "        # Load and resample the images\n",
    "        t1 = self._resample_image(t1_path)\n",
    "        t1ce = self._resample_image(t1ce_path)\n",
    "        flair = self._resample_image(flair_path)\n",
    "        t2 = self._resample_image(t2_path)\n",
    "        seg = self._resample_image(seg_path)\n",
    "\n",
    "        # Resize the images to target dimensions (e.g., 128x128x128)\n",
    "        t1 = self._resize_image(t1)\n",
    "        t1ce = self._resize_image(t1ce)\n",
    "        flair = self._resize_image(flair)\n",
    "        t2 = self._resize_image(t2)\n",
    "        seg = self._resize_image(seg)\n",
    "\n",
    "        # Print the shape of the image after resampling and resizing\n",
    "        #print(f\"Image shape after resampling and resizing: {t1.shape}\")\n",
    "        \n",
    "        # Normalize the images (for each modality)\n",
    "        t1 = self._normalize_image(t1)\n",
    "        t1ce = self._normalize_image(t1ce)\n",
    "        flair = self._normalize_image(flair)\n",
    "        t2 = self._normalize_image(t2)\n",
    "        \n",
    "        # Stack the images and segmentation into a single tensor\n",
    "        image = np.stack([t1, t1ce, flair, t2, seg], axis=0)\n",
    "\n",
    "        # Print the size of the image after resampling, resizing, and normalization\n",
    "        #print(f\"Image shape after resampling, resizing, and normalization: {image.shape}\")\n",
    "        \n",
    "        # Get the survival time for this patient\n",
    "        survival_time = self.patient_data[patient_id]['Survival']\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Convert image and survival time to torch tensors\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        survival_time = torch.tensor(survival_time, dtype=torch.float32)\n",
    "        \n",
    "        return image, survival_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the directory with images and segmentation\n",
    "image_dir = \"/Users/lindatang/Desktop/tumor_dl/BRATS2017/TrainingDataset/images\"\n",
    "\n",
    "# Define the path to the CSV file containing the patient survival data\n",
    "csv_path = \"/Users/lindatang/Desktop/tumor_dl/BRATS2017/survival_data.csv\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = GBMdataset(image_dir=image_dir, csv_path=csv_path)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'survival_times' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msurvival_times\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'survival_times' is not defined"
     ]
    }
   ],
   "source": [
    "# # Example loop over the data\n",
    "# for batch_idx, (images, survival_times) in enumerate(dataloader):\n",
    "#     if batch_idx < 2:  # Only print for the first 2 batches\n",
    "#         print(f\"Batch {batch_idx + 1}\")\n",
    "#         print(f\"Images shape: {images.shape}\")\n",
    "#         print(f\"Survival times: {survival_times}\")\n",
    "#     else:\n",
    "#         break  # Exit the loop after the first 2 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tumor_dl/.venv/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tumor_dl/.venv/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tumor_dl/.venv/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGaussianNoise3D\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/tumor_dl/.venv/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tumor_dl/.venv/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision::nms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should have 4 elements in dimension 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class GaussianNoise3D(nn.Module):\n",
    "    def __init__(self, mean=0.0, std=0.1):\n",
    "        super(GaussianNoise3D, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:  # Apply noise only during training\n",
    "            noise = torch.randn_like(x) * self.std + self.mean\n",
    "            return x + noise\n",
    "        return x\n",
    "    \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_shape, network_depth, no_convolutions, conv_filter_no_init, \n",
    "                 conv_kernel_size, latent_representation_dim, l1, l2, dropout_value, \n",
    "                 use_batch_normalization, activation, gaussian_noise_std=None):\n",
    "        super(encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.network_depth = network_depth\n",
    "        self.no_convolutions = no_convolutions\n",
    "        self.conv_filter_no_init = conv_filter_no_init\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.latent_representation_dim = latent_representation_dim\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.dropout_value = dropout_value\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "        self.activation = activation\n",
    "        self.gaussian_noise_std = gaussian_noise_std   \n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "\n",
    "        # Gaussian noise layer\n",
    "        if gaussian_noise_std:\n",
    "            self.noise_layer = GaussianNoise3D(gaussian_noise_std)\n",
    "        else:\n",
    "            self.noise_layer = None\n",
    "\n",
    "        # Convolutional layers\n",
    "        in_channels = input_shape[0]\n",
    "        for i in range(network_depth):\n",
    "            for j in range(no_convolutions):\n",
    "                out_channels = self.conv_filter_no_init * (2 ** i)\n",
    "                conv_layer = nn.Conv3d(in_channels, out_channels, conv_kernel_size, padding=conv_kernel_size // 2)\n",
    "                self.encoder_layers.append(conv_layer)\n",
    "                if self.use_batch_normalization:\n",
    "                    self.encoder_layers.append(nn.BatchNorm3d(out_channels))\n",
    "                if self.activation == 'leakyrelu':\n",
    "                    self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                else:\n",
    "                    self.encoder_layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = out_channels\n",
    "            self.encoder_layers.append(nn.MaxPool3d(kernel_size=2, stride=2))\n",
    "            if dropout_value:\n",
    "                self.encoder_layers.append(nn.Dropout3d(p=dropout_value))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate feature map size after convolution\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            conv_output = self._forward_conv_layers(dummy_input)\n",
    "            self.feature_map_size = conv_output.size()\n",
    "            flattened_dim = conv_output.view(1, -1).size(1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(flattened_dim, latent_representation_dim)\n",
    "        if activation == 'leakyrelu':\n",
    "            self.activation_fn = nn.LeakyReLU(inplace=True)\n",
    "        else:\n",
    "            self.activation_fn = nn.ReLU(inplace=True)\n",
    "            \n",
    "    def _forward_conv_layers(self, x):\n",
    "        if self.noise_layer:\n",
    "            x = self.noise_layer(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_shape = (5, 128, 128, 128)  # 5 channels, 240x240x155 spatial dimensions\n",
    "network_depth = 4\n",
    "no_convolutions = 2\n",
    "conv_filter_no_init = 32\n",
    "conv_kernel_size = 3\n",
    "latent_representation_dim = 128\n",
    "l1 = 0.01\n",
    "l2 = 0.01\n",
    "dropout_value = 0.5\n",
    "use_batch_normalization = True\n",
    "activation = 'leakyrelu'\n",
    "gaussian_noise_std = 0.1\n",
    "\n",
    "encoder = encoder(\n",
    "    input_shape=input_shape,\n",
    "    network_depth=network_depth,\n",
    "    no_convolutions=no_convolutions,\n",
    "    conv_filter_no_init=conv_filter_no_init,\n",
    "    conv_kernel_size=conv_kernel_size,\n",
    "    latent_representation_dim=latent_representation_dim,\n",
    "    l1=l1,\n",
    "    l2=l2,\n",
    "    dropout_value=dropout_value,\n",
    "    use_batch_normalization=use_batch_normalization,\n",
    "    activation=activation,\n",
    "    gaussian_noise_std=gaussian_noise_std\n",
    ")\n",
    "\n",
    "print(encoder.feature_map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 16:54:30,721\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Can't import ray.tune as some dependencies are missing. Run `pip install \"ray[tune]\"` to fix.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/tumor_dl/.venv/lib/python3.12/site-packages/ray/tune/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## hyperparameter tuning\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint, get_checkpoint\n",
      "File \u001b[0;32m~/Desktop/tumor_dl/.venv/lib/python3.12/site-packages/ray/tune/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt import ray.tune as some dependencies are missing. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mray[tune]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` to fix.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExperimentAnalysis\n",
      "\u001b[0;31mImportError\u001b[0m: Can't import ray.tune as some dependencies are missing. Run `pip install \"ray[tune]\"` to fix."
     ]
    }
   ],
   "source": [
    "## hyperparameter tuning\n",
    "\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"l1\": tune.choice([2 ** i for i in range(9)]),\n",
    "    \"l2\": tune.choice([2 ** i for i in range(9)]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "}\n",
    "\n",
    "\n",
    "        self.conv_shape = conv_shape  # Shape of the feature map at the start of the decoder\n",
    "        self.network_depth = network_depth\n",
    "        self.no_convolutions = no_convolutions\n",
    "        self.conv_filter_no_init = conv_filter_no_init\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.latent_representation_dim = latent_representation_dim\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.dropout_value = dropout_value\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "        self.activation = activation\n",
    "        self.output_channels = output_channels  # Final output channels (e.g., 5 channels for MRI modalities)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3-0: in_channels = 256, out_channels = 512\n",
      "Layer 3-1: in_channels = 512, out_channels = 512\n",
      "Layer 2-0: in_channels = 512, out_channels = 256\n",
      "Layer 2-1: in_channels = 256, out_channels = 256\n",
      "Layer 1-0: in_channels = 256, out_channels = 128\n",
      "Layer 1-1: in_channels = 128, out_channels = 128\n",
      "Layer 0-0: in_channels = 128, out_channels = 64\n",
      "Layer 0-1: in_channels = 64, out_channels = 64\n",
      "Final Layer: in_channels = 64, out_channels = 5\n",
      "Decoder output shape: torch.Size([1, 5, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def downsample_fn(depth):\n",
    "    if depth == 4:\n",
    "        return [(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)]  # Increase depth upsample factor to 2\n",
    "    elif depth == 2:\n",
    "        return [(4, 4, 4), (4, 4, 4)]  # Increase depth upsample factor to 4 if needed\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported depth: {depth}')\n",
    "    \n",
    "class Decoder3D(nn.Module):\n",
    "    def __init__(self, conv_shape, network_depth, no_convolutions, conv_filter_no_init,\n",
    "                 conv_kernel_size, latent_representation_dim, output_channels=5, l1=0.0, l2=0.0,\n",
    "                 dropout_value=0.0, use_batch_normalization=False, activation='relu'):\n",
    "        super(Decoder3D, self).__init__()\n",
    "        self.conv_shape = conv_shape  # Shape of the feature map at the start of the decoder\n",
    "        self.network_depth = network_depth\n",
    "        self.no_convolutions = no_convolutions\n",
    "        self.conv_filter_no_init = conv_filter_no_init\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.latent_representation_dim = latent_representation_dim\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.dropout_value = dropout_value\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "        self.activation = activation\n",
    "        self.output_channels = output_channels  # Final output channels (e.g., 5 channels for MRI modalities)\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'leakyrelu':\n",
    "            self.activation_fn = nn.LeakyReLU(inplace=True)\n",
    "        else:\n",
    "            self.activation_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Fully connected layer to reshape the latent vector into a 3D shape\n",
    "        self.fc = nn.Linear(latent_representation_dim, np.prod(self.conv_shape))\n",
    "\n",
    "        # Reshape layer to convert the flat output of the FC layer into a 3D volume\n",
    "        self.reshape = lambda x: x.view(-1, *self.conv_shape)\n",
    "        \n",
    "        # Decoder layers (upsample and conv layers)\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        # Reverse the depth, so we progressively upsample back to the original image size\n",
    "        in_channels = conv_shape[0]  # Start with the number of channels from the conv_shape\n",
    "        for i in reversed(range(network_depth)):\n",
    "            # Upsampling layer\n",
    "            upsample_factors = downsample_fn(network_depth)[i]\n",
    "            self.decoder_layers.append(nn.Upsample(scale_factor=upsample_factors, mode='trilinear', align_corners=False))\n",
    "            \n",
    "            # Convolution layers\n",
    "            out_channels = self.conv_filter_no_init * (2 ** i)  # Reduce the number of channels as we move up the network\n",
    "            for j in range(no_convolutions):\n",
    "                print(f\"Layer {i}-{j}: in_channels = {in_channels}, out_channels = {out_channels}\")\n",
    "                self.decoder_layers.append(nn.Conv3d(in_channels, out_channels, kernel_size=conv_kernel_size, padding=1))\n",
    "                if use_batch_normalization:\n",
    "                    self.decoder_layers.append(nn.BatchNorm3d(out_channels))\n",
    "                if activation == 'leakyrelu':\n",
    "                    self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "                else:\n",
    "                    self.decoder_layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = out_channels  # Update in_channels for the next layer\n",
    "            if dropout_value > 0.0:\n",
    "                self.decoder_layers.append(nn.Dropout3d(p=dropout_value))\n",
    "        \n",
    "        # Final convolution to produce the reconstructed image with the correct number of output channels\n",
    "        print(f\"Final Layer: in_channels = {in_channels}, out_channels = {self.output_channels}\")\n",
    "        self.final_conv = nn.Conv3d(in_channels, self.output_channels, conv_kernel_size, padding=1)\n",
    "        self.final_activation = nn.ReLU()  # You could change this to `nn.Sigmoid()` or `nn.Tanh()` depending on the data range\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the latent vector via the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = self.activation_fn(x)\n",
    "        \n",
    "        # Reshape to the shape required for the convolutional layers\n",
    "        x = self.reshape(x)  # Reshape to 3D tensor (batch_size, channels, depth, height, width)\n",
    "        \n",
    "        # Apply the decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Final convolution to produce the output volume\n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_activation(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "latent_vector = torch.randn(1, 128)  # Latent vector of size [batch_size, latent_representation_dim]\n",
    "conv_shape = (256, 8, 8, 8)  # Example shape of the feature map before the decoder\n",
    "decoder_model = Decoder3D(conv_shape=conv_shape, network_depth=4, no_convolutions=2, \n",
    "                          conv_filter_no_init=64, conv_kernel_size=3, \n",
    "                          latent_representation_dim=128)\n",
    "\n",
    "output_image = decoder_model(latent_vector)\n",
    "print(\"Decoder output shape:\", output_image.shape)  # Should be [batch_size, 5, 240, 240, 150] or similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentParametersModel(nn.Module):\n",
    "    def __init__(self, latent_representation_dim, l1=0.0, l2=0.0):\n",
    "        super(LatentParametersModel, self).__init__()\n",
    "        self.mu_sigma_layer = nn.Linear(\n",
    "            in_features=latent_representation_dim, \n",
    "            out_features=2\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.mu_sigma_layer.weight)\n",
    "        nn.init.zeros_(self.mu_sigma_layer.bias)\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu_sigma = self.mu_sigma_layer(x)\n",
    "        return mu_sigma\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted mu and sigma: tensor([[ 1.1835, -2.3457]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "latent_representation_dim = 128  # Example latent representation dimension\n",
    "model = LatentParametersModel(latent_representation_dim=latent_representation_dim)\n",
    "\n",
    "# Simulate an input from the encoder\n",
    "x = torch.randn((1, latent_representation_dim))  # Batch size of 1, with 128-dimensional latent vector\n",
    "mu_sigma = model(x)\n",
    "\n",
    "print(\"Predicted mu and sigma:\", mu_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4084, 0.2187, 0.4220])\n"
     ]
    }
   ],
   "source": [
    "def reconstruction_loss(y_true, y_pred):\n",
    "    mse_loss = F.mse_loss(y_pred, y_true, reduction='none')\n",
    "    reduced_loss = mse_loss.mean(dim=[1, 2, 3])\n",
    "    return reduced_loss\n",
    "\n",
    "import torch\n",
    "\n",
    "def survival_loss(mu, sigma, x, delta):\n",
    "    \"\"\"\n",
    "    Custom loss function based on the negative log-likelihood.\n",
    "\n",
    "    :param mu: Predicted mean (log of hazard ratio), tensor of shape (batch_size,)\n",
    "    :param sigma: Predicted standard deviation (scale parameter), tensor of shape (batch_size,)\n",
    "    :param x: Observed time (log-transformed), tensor of shape (batch_size,)\n",
    "    :param delta: Event indicator (1 if event occurred, 0 if censored), tensor of shape (batch_size,)\n",
    "    :return: Computed loss, scalar value\n",
    "    \"\"\"\n",
    "    # Negative log-likelihood term\n",
    "    total_loss = -(torch.log(x)-mu)/sigma.sum()+(delta * torch.log(sigma) + (1 + delta) * torch.log(1 + torch.exp((torch.log(x)-mu)/sigma)))\n",
    "    \n",
    "    # Return the mean loss across the batch\n",
    "    return total_loss / x.size(0)\n",
    "\n",
    "# Example usage\n",
    "mu = torch.tensor([0.5, 0.8, 0.3])  # Predicted means (log hazard ratios)\n",
    "sigma = torch.tensor([1.1, 1.2, 1.1])  # Predicted standard deviations (not log-transformed)\n",
    "x = torch.tensor([1.0, 0.8, 0.9])  # Log-transformed observed times\n",
    "delta = torch.tensor([1.0, 0.0, 1.0])  # Event indicators\n",
    "\n",
    "loss = survival_loss(mu, sigma, x, delta)\n",
    "print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
